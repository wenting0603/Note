# 3.資料探勘 Data mining
機器學習與深度學習
有三大方向
1.關聯分析
2.分類
3.預測
#### 差別
機器學習是只運用我們給予的演算法，計算後回復答案(沒有學習)
深度學習會運用給予的資料再去延伸，會把自己修正的更好

### Business Intelligent
主要是視覺化，無預測

#### 監督式&非監督
**監督式:有Y(自變數) 要預測Y就是監督式學習(有標準答案)**
例如鐵達尼數據中是否生存就是重要的想得知的資訊
不同的X下 Y會是什麼
預測性

非監督式:沒有明確的預測目標的
例如無答案的試題
每一次都是未知
描述性

####  維度DIMANTION
就是欄位,資料庫中所需要的feature(如性別 學號 身高 體重),是總資料庫精簡成自己所需的子集合

一個公司中有許多部門，行銷部 客服部 公關部 採購部等等...
他們所需的資料就不一樣，他們的維度就不同

## Data Mining的定義
1.資料量要越大，最好呈現非線性(線性又少量就職節使用統計就好)
2.半自動化:要知道演算法的算法 優勢 等，調整想探勘的資料，才能去符合inpute

### 使用資料探勘的意義
1.資料過於複雜與龐大
2.把資料轉換為知識
![](https://i.imgur.com/UkoFlQt.png)
    將資料庫的數據化維可理解的資訊，如最大最小值 平均值 圖示化
    但將資訊轉換為知識是需要了解資訊的規則

### 與統計的不同
#### 統計:
1.用線性的方式做預測 
**X->Y**
認真考試就會有高分 包牌包很多就會重大樂透
但並非一定的，因此**現實中有許多問題都是非線性的**

2.樣本太大會造成太多誤差，因此**樣本不能過大**

## WEKA
### arff檔案形式
```bash=
@RELATION wine '''專案名稱'''

'''@ATTRIBUTE 特徵值    是否為連續型資料 '''
@ATTRIBUTE class	{1,2,3}
@ATTRIBUTE alcohol	REAL
@ATTRIBUTE malicacid	REAL
@ATTRIBUTE ash	REAL
@ATTRIBUTE alcalinityofash	REAL
@ATTRIBUTE magnesium	REAL
@ATTRIBUTE totalphenols	REAL
@ATTRIBUTE flavanoids	REAL
@ATTRIBUTE nonflavanoidphenols	REAL
@ATTRIBUTE proanthocyanins	REAL
@ATTRIBUTE colorintensity	REAL
@ATTRIBUTE hue	REAL
@ATTRIBUTE od280/od315ofdilutedwines	REAL
@ATTRIBUTE proline	REAL


@DATA
'''特徵值的數據'''
1,14.23,1.71,2.43,15.6,127,2.8,3.06,.28,2.29,5.64,1.04,3.92,1065
1,13.2,1.78,2.14,11.2,100,2.65,2.76,.26,1.28,4.38,1.05,3.4,1050
1,13.16,2.36,2.67,18.6,101,2.8,3.24,.3,2.81,5.68,1.03,3.17,1185

```
## 關聯法則（非監督是學習）

***目的是在一個資料集當中，找出不同項與項之間可能存在的關係　來建立商品推薦***
指標值的門檻設定，會決定關聯的強弱
兩項指標:support confidence

Ｘ－＞Ｙ
推薦的Ｙ是未知
因此為非監督

離散資料　－＞離散資料

### 支持度 Support
**某項目集在資料庫中出現的次數比例**
某資料庫中有100筆交易紀錄，其中有20筆交易有購買啤酒，則啤酒的支持度為20%
支持度越高代表項目更值得深入探討

### 信賴度 Confidence
**兩個項目集之間的條件機率**，也就是在A出現的情況下，B出現的機率值
![](https://i.imgur.com/XE8Vz0T.png)

兩項指標都高是最好，但信賴度高是最好的

#### 舉例
有四張發票有ABCDE種商品
設定min_support=50%
min_confidence = 80%
![](https://i.imgur.com/UZmHpzo.png)

4*50%=2筆的資料可視為Large itemsets
也就是商品有在2張發票中出現才大於最小支持度
![](https://i.imgur.com/Sm7CfJI.png)

**要有組合才會有推薦**
因此要湊出多階組合
**２階**
![](https://i.imgur.com/xjNCrs1.png)

**３階**
![](https://i.imgur.com/PB8nY2V.png)
因為低階有組合沒過，就不成立更高階
ＡＣＢ－＞ＡＢ（Ｘ）　ＡＣＥ－＞ＡＥ（Ｘ）

接著看ｍｉｎ＿ｃｏｎｄｉｄｅｎｃｅ有沒有過８０％
![](https://i.imgur.com/jQBtsGb.png)
![](https://i.imgur.com/nUYBCOm.png)

![](https://i.imgur.com/Fp9FvIO.png)

### WEKA關聯分析
Iris 為例
關聯分析是:離散資料　－＞離散資料
要將連續型的資料作離散

1. 在Preprocess中選chose 選擇unsupervise中的Discretize
![](https://i.imgur.com/blBEcxQ.png)

2. 按左鍵可以叫出參數更改分組數量(bin)
![](https://i.imgur.com/vjUBSe8.png)
3. apply後就可以得到離散的資料
![](https://i.imgur.com/PQJiZLm.png)

5. 按下ASSOCIATE中選擇關聯性的方程式![](https://i.imgur.com/hCWKgUQ.png)
6. 可以自行調整support與confidence的數值直到有滿意的結果![](https://i.imgur.com/W1gtJ00.png)


## 分類規則(監督式->有標準答案)

* Classification分類[Predictive]
* Clustering分群[Descriptive]

###  Classification分類
建一個分類模型(已知資料建模),再用這個Model(分類器)對新資料做預測
![](https://i.imgur.com/T9cAaHq.png)

### 決策樹
料結構中的樹狀結構相仿,皆擁有根(Root)、節點(Node)以及樹葉(Leaf)等結構

### 貝氏網路分類
以機率去分類
由節點(Nodes)與連結線(Edges)所組成,而每個node都有
一組狀態機率表 (Condition Probability Tables)

### KNN(K-nearest neighbors algorithm)
簡單來說就是**西瓜偎大邊** 多數決
看附近多數是什麼就分成該群

![](https://i.imgur.com/nNlTUex.png)
**附近有8個點 6綠2紅 綠的多就加入綠色**
當各種類別數量一樣時就隨便選一邊

KNN十分直覺，但要控管的參數有:
1. K :為一個常數 就是要掃描幾個附近的點
2. 距離:怎麼算出資料的「距離」,例如座標距離、顏色相近程度、這個會跟你的資料還有分類方法有關。

每次都要掃描全部的資料算出距離,才能知道最近的k筆資料是什麼,需要相對高的記憶體跟計算時間

### 決策樹
**優點**
– 分類出的規則容易理解
– 資料運算處理時間不會太長
– 可處理**類別型**的字串資料型態
**缺點**
– **較難處理連續型**的字串資料型態:需做離散去切割1-10離散成低中高
– 處理時間序列型的資料型態需先進行離散化
– 資料類別太多時,錯誤率快速提升

#### 屬性的選擇指標
好的屬性帶你上天堂
**常用的屬性選擇指標有:**
○ 資訊獲利 (Information Gain) – ID3、C4.5、C5.0
○ 吉尼係數 (Gini Index) – CART

#### Entropy(熵)
**衡量資訊量**
愈純淨的資料所需要的資訊量愈少,反之愈雜亂的資料需要
的資訊量愈多。
![](https://i.imgur.com/H1zKJl2.png)
在分割後屬性量也會不同
![](https://i.imgur.com/w3LlxrW.png)

**選擇具有最大資訊增益量的屬性就會成為node**
![](https://i.imgur.com/4o5myx7.png)

1. 計算未拆分前的entropy
2. 依照各個屬性拆分
3. 計算各屬性拆分後的entropy
4. 用原本的熵-屬性的熵(GAIN)
6. 最高的就會做為分支點依據


### overfitting

太符合訓練資料，導至在測試時不符合
* 屬性太多:演算法剛好選擇到和類別不相關的屬性
* 偏差(Bias) :有時會出現決策樹只對某一訓練資料集有效,更換另一組訓練資料集


![](https://i.imgur.com/0Lt80XV.png)
**不用上所有的feature**，當答案都歸屬到同一類別不須將所有屬性都用上

![](https://i.imgur.com/6iv9FPe.png)

### 隨機森林
這些決策樹是隨機選擇屬性。
之後使用多數決來決定答案
![](https://i.imgur.com/G8UMJDD.png)
這種多數決的方式就稱之為　**集成學習**
![](https://i.imgur.com/kj2Qks7.png)

#### bagging
從資料集中取出樣本來
拿K個建完樹後再放回去，在拿K個在建樹
![](https://i.imgur.com/LVR0awh.png)


#### boosting
把錯誤的地方從新建樹
![](https://i.imgur.com/Ku8638h.png)

### cross validation
![](https://i.imgur.com/oOtYU4C.png)
分布同等分，其中一份作訓練 一部份作測驗
每次都拿不同等分

### Confusion Matrix
![](https://i.imgur.com/4KkEAkt.png)
![](https://i.imgur.com/1PJCiOp.png)

都想看的時候就使用F
![](https://i.imgur.com/TJ8JuK7.png)

### Percentage split
有使用幾%作為訓練資料

## 群集分析法(非監督式->無標準答案)

1. 資料精簡:先分群後 在從各裙裡抽樣本
2. 推斷假設的產生– 推斷出所關注資料中可能存在的某些特性或現象
3. 推斷假設的驗證
4. 歸屬預測

#### 主要步驟
![](https://i.imgur.com/gq3xBVG.png)
**相似度衡量是群集最重要的部分**

### 分割式群集分析法
–為目前進行群集分析的主流方式
-K-means
![](https://i.imgur.com/nUDp0Uk.png)
**最小距離總偏移值**
群中各點與群中心的距離是最小(與群外的點相比)
**反覆重新配置技術**
分割式分群法一開始會**隨機**先選擇k個資料點當作起始之群集中心
每次都會更新群中心，直到達到最小的距離總偏移值  
  
  

●Step1: 給n個物件,初始化k個群中心。
●Step2: 將每一個物件分派到最靠近它的群。
![](https://i.imgur.com/u2ldAX0.png)

●Step3: 更新群的中心點。
●Step4: 重複步驟2和步驟3,直到群不再變化。  
  
優點:
* 概念與實作上相當的簡單
* 大量資料時相當有擴充性且有效率 
  
  
缺點
* 必須為數值資料(計算平均值)
* 先決定群集的數目K
* 容易受到雜質或離群值影響群集的結果

### 階層式群集分析法
– 分類樹狀圖



















