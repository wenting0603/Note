# 4.網路爬蟲
三步驟
1. 請求指定的URL以取得回應正文
1. 解析回應正文內容並從中分析所需資訊
1. 將上一部分析的資訊儲存到資料庫或檔案中


## 1.Requests模組: 讀取網站檔案
**步驟一:請求指定的URL以取得回應正文**
python的模組可以用來讀取網站檔案

Requests 預設的讀取編碼為Latin-1,如果讀取的頁面編碼不同,常常會造成亂碼的產生。可以設定Response物件的編碼為UTF-8或Big5  

轉換編碼
```
html.encoding = 'UTF-8'
```
### GET
當打開瀏覽器後輸入網址送出,指定的網站伺服器接到要求後回應內容,可在瀏覽器中看到網頁的呈現  

**status_code**:取得HTTP狀態碼
用來確任連線狀態
* 資訊回應 (Informational responses, 100–199),
* 成功回應 (Successful responses, 200–299),
* 重定向 (Redirects, 300–399),
* 用戶端錯誤 (Client errors, 400–499),
* 伺服器端錯誤 (Server errors, 500–599).

```bash=
payload = {'key1':'value1','key2':'value2'}
r = requests.get("http://httpbin.org/get",params = payload)
###檢查HTTP的回覆是否為200(成功回應)
if r.status_code ==requests.codes.ok:
    print(r.text)
```
輸出:
```
{
  "args": {
    "key1": "value1", 
    "key2": "value2"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.25.1", 
    "X-Amzn-Trace-Id": "Root=1-61ea2c35-04c3df7f2fbe240918389ab3"
  }, 
  "origin": "123.192.82.19", 
  "url": "http://httpbin.org/get?key1=value1&key2=value2"
}
```
### POST
只要是網頁中有**讓使用者填入資料的表單**,都會需要用POST請求來進行傳送
• 在requests模組中,POST傳遞的參數要定義成**字典**資料型態,接著用POST請求時必須將傳遞的**參數內容設定為
data參數**
```bash=
payload = {'key1':'value1','key2':'value2'}
r = requests.post("http://httpbin.org/post",data = payload)
print(r.text)
```
輸出:
```
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "key1": "value1", 
    "key2": "value2"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "23", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.25.1", 
    "X-Amzn-Trace-Id": "Root=1-61ea3fa6-67e6ee5a17db0f74257338be"
  }, 
  "json": null, 
  "origin": "123.192.82.19", 
  "url": "http://httpbin.org/post"
}
```

## 2.BeautifulSoup模組: 網頁解析
步驟二:**解析**回應正文內容並從中分析所需資訊  
### html格式
HTML語法利用標籤(tag)建構內容
所有標籤都是由<.....>包含,大部分都有起始與結束標籤
```
– <h1>標題</h1>
– <div>區塊內容</div>
– <p>段落</p>
– <img>圖片</img>
– <a>超連結</a>
```
![](https://i.imgur.com/i3VMi1K.png)
BeautifulSoup模組的功能將讀取的網頁原始碼解析為一個個結構化的物件,讓程式能夠快速取得其中的內容。  
  
### BeautifulSoup的使用  

利用requests模組取得網頁原始碼,就可以使用”lxml”解析原始碼  
```bash=
變數 = BeautifulSoup(html.text'''原始碼'''),'html.parser''''解析器''')
```

```bash=
url = 'https://sites.google.com/site/chiayulin0121/'
html = requests.get(url)
html.encoding = 'UTF-8'
sp = BeautifulSoup(html.text,'html.parser')
```
原始碼:requests扒下來的html內容
解析器:
1.“lxml”是一個基於C語言的解析器,執行速度更快(要安裝 pip install lxml)
2.”html.parser”是Python內建的解析器

```bash=
###　解析完成的變數.html的tag->顯示tag與tag中的內容
print(sp.title)
> <title>chiayulin</title>
### 解析完成的變數.html的tag.text->顯示tag中的內容
print(sp.title.text)
> chiayulin
```   

### Find
加入標籤屬性做搜尋條件
```bash=
#解析完成的物件.find()->尋找第一個符合條件的標籤，以字串回傳
print(sp.find('p'))
> <p id="p1">我是段落一</p>
#解析完成的物件.find_all()->尋找所有符合條件的標籤，以串列回傳
print(sp.find_all('p'))
> [<p id="p1">我是段落一</p>, <p class="red" id="p2">我是段落二</p>]
## 參數指定 2種方法都可以
# 1.find(物件,id='指定標籤', class_='某種類別'))
print(sp.find('p',id='p2', class_='red'))
> <p class="red" id="p2">我是段落二</p>
#2.find(物件,{'id':'指定標籤', 'class':'某種類別'}))
print(sp.find('p',{'id':'p2','class':'red'}))
> <p class="red" id="p2">我是段落二</p>
``` 
```
html = '''
<html>
<head>
<meta charset="UTF-8">
<title>我是網頁標題</title>
</head>
<body>
<div>
<p id="p1">我是段落一</p>
<p id="p2" class='red'>我是段落二</p>
</div>
</body>
</html>
'''
```
### Select
尋找指定**CSS**選擇器如id或class的內容，以串列回傳

**以id讀取**:sp.select(“#id”)
```bash=
print(sp.select('#p1'))
> [<p id="p1">我是段落一</p>]
```
**以class讀取**:sp.select(“.classname”)
```bash=
print(sp.select('.red'))
> [<p class="red" id="p2">我是段落二</p>]
```
#### 圖片 超連結讀取
```
html = '''
<html>
<head>
<meta charset="UTF-8">
<title>我是網頁標題</title>
</head>
<body>
<img src="http://www.ehappy.tw/python.png">
<a href="http://www.e-happy.com.tw">超連結</a>
</body>
</html>
'''
```
兩種方式:
1.用物件.select('ID')][0].get('class')
2.用物件.select('ID')][0]['class']
```bash=
sp = BeautifulSoup(html,'html.parser')
#物件.select('ID')][0].get('類型')
print(sp.select('img')[0].get('src'))
print(sp.select('a')[0].get('href'))
#物件.select('ID')][0]['類型']
print(sp.select('img')[0]['src'])
print(sp.select('a')[0]['href'])
```

## 3.Chrome 開發人員工具
觀察網頁的結構
用Chrome的「開發人員工具」來協助。
![](https://i.imgur.com/i7f3QEg.png)
於想知道的部分按下左鍵
就會標出此部分的程式碼
![](https://i.imgur.com/rlwHVsE.png)


## 4.抓出威力彩的號碼
步驟:
1. 開發人員工具觀察網頁的結構
2. 撰寫爬蟲程式

**目標**
![](https://i.imgur.com/wy4KHUH.png)
從網頁->文字
![](https://i.imgur.com/0j7sTKh.png)  

### 1.檢查網頁的html語法
![](https://i.imgur.com/dZ2xj3L.png)
```
<div class="contents_box02">
      <div id="contents_logo_02"></div><div class="contents_mine_tx02"><span class="font_black15">111/1/20&nbsp;第111000006期 </span><span class="font_red14"><a href="Result_all.aspx#01">開獎結果</a></span></div><div class="contents_mine_tx04">開出順序<br>大小順序<br>第二區</div><div class="ball_tx ball_green">38 </div><div class="ball_tx ball_green">29 </div><div class="ball_tx ball_green">23 </div><div class="ball_tx ball_green">02 </div><div class="ball_tx ball_green">05 </div><div class="ball_tx ball_green">09 </div><div class="ball_tx ball_green">02 </div><div class="ball_tx ball_green">05 </div><div class="ball_tx ball_green">09 </div><div class="ball_tx ball_green">23 </div><div class="ball_tx ball_green">29 </div><div class="ball_tx ball_green">38 </div><div class="ball_red">02 </div>
    </div>
```
### 2.將範圍縮小到威力彩區
```bash=
#<div class="contents_box02">維威力彩開始著區域
#id = div class_=contents_box02
sp_w=(sp.find('div', class_='contents_box02'))
```
### 3.找出當期號碼
```bash=
#<span class="font_black15">111/1/20&nbsp;第111000006期 </span>
#id = span class_=contents_box02
sp_d=(sp_w.find('span', class_='font_black15'))
#運用string只保留tag中的內容
date = sp_d.string
```
### 4.獲取數字
因為有多組，**要使用find_all**
![](https://i.imgur.com/JdqQdlc.png)

```bash=
sp_n=(sp_w.find_all('div', class_='ball_tx ball_green'))
nums=[]
#用for迴圈將個數字取出
for n in sp_n:
    nums.append(n.string)#運用string只保留tag中的內容
#將陣列轉換成文字
n_1 = "".join(nums[0:6])
n_2 = "".join(nums[6:12])

#特殊號
sp_r=(sp_w.find('div', class_='ball_red'))
n_3 = sp_r.string
```
### 5.將所有資訊匯集並印出
```bash=
print(f"威力彩期數 : {date}\n開出順序: {n_1}\n大小順序: {n_2}\n第二區: {n_3}")

```
**結果:**

威力彩期數 : 111/1/20 第111000006期 
開出順序: 38 29 23 02 05 09 
大小順序: 02 05 09 23 29 38 
第二區: 02 

#### 完整code

```bash=
###威力彩爬蟲
import requests
from bs4 import BeautifulSoup
url = 'https://www.taiwanlottery.com.tw/index_new.aspx'
html = requests.get(url)
html.encoding = 'UTF-8'
sp = BeautifulSoup(html.text,'html.parser')

#利用find()、<div>、class找出威力彩的區塊
sp_w=(sp.find('div', class_='contents_box02'))


#利用find()、<span>、class找出威力彩期數
sp_d=(sp_w.find('span', class_='font_black15'))
#sp_d.clear()
date = sp_d.string


#利用find_all()找出開獎號碼
sp_n=(sp_w.find_all('div', class_='ball_tx ball_green'))
nums=[]
for n in sp_n:
    nums.append(n.string)
n_1 = "".join(nums[0:6])
n_2 = "".join(nums[6:12])


#特殊號
sp_r=(sp_w.find('div', class_='ball_red'))
n_3 = sp_r.string

print(f"威力彩期數 : {date}\n開出順序: {n_1}\n大小順序: {n_2}\n第二區: {n_3}")
```


